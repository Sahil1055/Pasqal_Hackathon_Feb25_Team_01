# -*- coding: utf-8 -*-
"""clinical_data_embeddings.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1FQWu5w79MEBun420UrttKRQHQb7Eb2jT

# Clinical Data Embeddings
"""

# Importing necessary libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import shap
import torch
import torch.nn as nn
import torch.optim as optim

from sklearn.preprocessing import LabelEncoder, StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import RFECV
from sklearn.model_selection import train_test_split

from sklearn.metrics.pairwise import cosine_similarity
from sklearn.decomposition import PCA

# Loading the dataset
df = pd.read_csv("clinical_Data.csv")

print("Dataset Overview:\n")
print("Shape of DataFrame:", df.shape)  # Number of rows and columns

print("\nColumn Names:\n", df.columns.tolist())  # Column names

print("\nData Types:\n", df.dtypes)  # Data types of each column

print("\nFirst 5 Rows:\n")  # First 5 rows
df.head()

# Summary Statistics
print("\nStatistical Summary:\n")  # Summary stats
df.describe(include='all')

# Missing Values Check
print("\nMissing Values Count:\n")  # Count of missing values per column
df.isna().sum()

print("\nTotal Missing Values:")  # Total missing values
df.isna().sum().sum()

# Check for Duplicates
print("\nDuplicate Rows Count:")  # Count of duplicate rows
df.duplicated().sum()

# Unique Values per Column
print("\nUnique Values Count:\n")  # Unique values per column
df.nunique()

# Defining the target variable [Censored_0_progressed_1 â†’ 1: Indicates liver cancer and 0: Indicates no progression or absence of liver cancer]
target_column = "Censored_0_progressed_1"
y = df[target_column]
df = df.drop(columns=[target_column])  # Removing target from features

# Encoding categorical variables
categorical_cols = df.select_dtypes(include=["object"]).columns
for col in categorical_cols:
    df[col] = LabelEncoder().fit_transform(df[col].astype(str))

# Handling missing values (fill with mean)
df.fillna(df.mean(), inplace=True)

# Normalizing the data
scaler = StandardScaler()
df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)

#Computing Feature Importance using Random Forest
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(df_scaled, y)

# feature importances
feature_importance = pd.Series(rf.feature_importances_, index=df_scaled.columns).sort_values(ascending=False)

# Plotting Feature Importance
plt.figure(figsize=(12, 12))
sns.barplot(x=feature_importance.values, y=feature_importance.index)
plt.title("Feature Importance (Random Forest)")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

#  Checking the features having low importance (less than 0.005) in contributing to the target variable
low_importance_threshold = 0.005
low_importance_features = feature_importance[feature_importance < low_importance_threshold].index.tolist()

print("Features to be dropped due to low importance:", low_importance_features)

# Dropping features that have very low importance
df_selected = df_scaled.drop(columns=low_importance_features)

# Recursive Feature Elimination (RFE)
rfe = RFECV(estimator=rf, step=1, cv=5, scoring='accuracy')
rfe.fit(df_selected, y)

# Computing correlation matrix
corr_matrix = df_selected.corr()

# Setting threshold for high correlation
threshold = 0.9

# Finding highly correlated features
high_corr_pairs = set()
for i in range(len(corr_matrix.columns)):
    for j in range(i):
        if abs(corr_matrix.iloc[i, j]) > threshold:
            high_corr_pairs.add((corr_matrix.columns[i], corr_matrix.columns[j]))

# Extracting features to drop (keeping one from each correlated pair)
features_to_drop = set([pair[1] for pair in high_corr_pairs])

# Dropping highly correlated features
df_final_reduced = df_selected.drop(columns=features_to_drop)

#correlation heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(corr_matrix, cmap="coolwarm", annot=False)
plt.title("Feature Correlation Matrix")
plt.show()

#list of dropped features
features_to_drop

df_selected_normalized = scaler.fit_transform(df_final_reduced)
# Converting to PyTorch tensor
patient_data = torch.tensor(df_selected_normalized, dtype=torch.float32)  # Shape: (105, num_features)
target_labels = torch.tensor(y.values, dtype=torch.long)  # Shape: (105,) for classification

# Normalize the selected features
scaler = StandardScaler()
df_selected_normalized = scaler.fit_transform(df_selected_normalized)

# Convert data to PyTorch tensors
patient_data = torch.tensor(df_selected_normalized, dtype=torch.float32)  # Shape: (105, num_features)
target_labels = torch.tensor(y.values, dtype=torch.long)  # Shape: (105,)

# Train-test split (90% train, 10% test)
train_data, test_data, train_labels, test_labels = train_test_split(
    patient_data, target_labels, test_size=0.1, random_state=42, stratify=target_labels
)

# PVEM Model
class PVEMClassifier(nn.Module):
    def __init__(self, input_dim, embedding_dim=128, num_classes=2):
        super(PVEMClassifier, self).__init__()
        self.weight = nn.Parameter(torch.randn(input_dim, embedding_dim))  # Learnable feature embeddings
        self.bias = nn.Parameter(torch.randn(input_dim, embedding_dim))  # Feature-wise bias
        self.dropout = nn.Dropout(p=0.3)  # Dropout for regularization
        self.classifier = nn.Linear(input_dim * embedding_dim, num_classes)  # Classifier

    def forward(self, x):
        embeddings = x.unsqueeze(-1) * self.weight + self.bias  # (batch_size, num_features, 128)
        flattened_embeddings = embeddings.reshape(x.shape[0], -1)  # (batch_size, num_features * 128)
        flattened_embeddings = self.dropout(flattened_embeddings)  # Apply dropout
        logits = self.classifier(flattened_embeddings)  # (batch_size, 2)
        return embeddings, logits

# Initialize model
num_features = train_data.shape[1]
embedding_dim = 128
model = PVEMClassifier(num_features, embedding_dim, num_classes=2)

# Define loss function & optimizer
criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Train model
num_epochs = 500
for epoch in range(num_epochs):
    optimizer.zero_grad()
    embeddings, logits = model(train_data)
    loss = criterion(logits, train_labels)
    loss.backward()
    optimizer.step()

    if epoch % 50 == 0:
        print(f"Epoch {epoch}, Loss: {loss.item():.6f}")

# Evaluate on Test Data
with torch.no_grad():
    test_embeddings, test_logits = model(test_data)
    test_loss = criterion(test_logits, test_labels)
    predictions = torch.argmax(test_logits, dim=1)
    accuracy = (predictions == test_labels).float().mean().item()

print(f"Test Loss: {test_loss.item():.6f}, Test Accuracy: {accuracy * 100:.2f}%")

# ðŸ”¹ Generate embeddings for the FULL dataset (all 105 patients)
with torch.no_grad():
    full_embeddings, _ = model(patient_data)  # Get embeddings for all patients

# Convert and save full embeddings
full_embeddings_np = full_embeddings.detach().cpu().numpy()

print("Full dataset embeddings saved successfully! Shape:", full_embeddings_np.shape)

print("Embeddings Generated Successfully!")
print("Final Shape:", full_embeddings_np.shape)  # Expected: (105, num_features, 128)

full_embeddings_np.mean()

full_embeddings_np.std()

full_embeddings_np.shape

print("Embedding of first patient (first feature):\n", full_embeddings_np[0, 0])  # Shape: (128,)
print("Embedding of first patient (second feature):\n", full_embeddings_np[0, 1])  # Shape: (128,)

plt.figure(figsize=(12, 6))
sns.heatmap(full_embeddings_np[0], cmap="coolwarm", annot=False)
plt.title("Heatmap of First Patient's Feature Embeddings")
plt.xlabel("Embedding Dimension (128)")
plt.ylabel("Feature Index")
plt.show()

# Flatten embeddings to (105, num_features * 128)
flat_embeddings = full_embeddings_np.reshape(105, -1)

# Compute similarity matrix
similarity_matrix = cosine_similarity(flat_embeddings)

# Extract only the first 5 patients' similarities
similarity_subset = similarity_matrix[:5, :5]

# Plot heatmap
plt.figure(figsize=(6, 4))  # Adjust size
ax = sns.heatmap(similarity_subset, annot=True, cmap="coolwarm", linewidths=0.5, fmt=".2f")

# Labels
plt.title("Cosine Similarity Heatmap (First 5 Patients)", fontsize=12)
plt.xticks(ticks=range(5), labels=[f"P{i+1}" for i in range(5)], fontsize=10)
plt.yticks(ticks=range(5), labels=[f"P{i+1}" for i in range(5)], fontsize=10, rotation=0)

plt.show()

# Saving embeddings as NumPy file
np.save("patient_feature_embeddings.npy", full_embeddings_np)

reshaped_data = full_embeddings_np.reshape(-1, 128)

# Creating a multi-index for (105, 38) patients & features
index = pd.MultiIndex.from_product([range(105), range(38)], names=["Patient", "Feature"])
df = pd.DataFrame(reshaped_data, index=index)

# Saving as CSV
df.to_csv("output_multiindex.csv")

print("CSV saved with shape:", df.shape)