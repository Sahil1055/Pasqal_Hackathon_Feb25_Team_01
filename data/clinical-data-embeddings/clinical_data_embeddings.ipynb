{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clinical Data Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import shap\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the dataset\n",
    "df = pd.read_csv(\"clinical_Data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Dataset Overview:\\n\")\n",
    "print(\"Shape of DataFrame:\", df.shape)  # Number of rows and columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nColumn Names:\\n\", df.columns.tolist())  # Column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nData Types:\\n\", df.dtypes)  # Data types of each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nFirst 5 Rows:\\n\")  # First 5 rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary Statistics\n",
    "print(\"\\nStatistical Summary:\\n\")  # Summary stats\n",
    "df.describe(include='all')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing Values Check\n",
    "print(\"\\nMissing Values Count:\\n\")  # Count of missing values per column\n",
    "df.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTotal Missing Values:\")  # Total missing values\n",
    "df.isna().sum().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for Duplicates\n",
    "print(\"\\nDuplicate Rows Count:\")  # Count of duplicate rows\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique Values per Column\n",
    "print(\"\\nUnique Values Count:\\n\")  # Unique values per column\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the target variable [Censored_0_progressed_1 â†’ 1: Indicates liver cancer and 0: Indicates no progression or absence of liver cancer]\n",
    "target_column = \"Censored_0_progressed_1\"\n",
    "y = df[target_column]\n",
    "df = df.drop(columns=[target_column])  # Removing target from features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoding categorical variables\n",
    "categorical_cols = df.select_dtypes(include=[\"object\"]).columns\n",
    "for col in categorical_cols:\n",
    "    df[col] = LabelEncoder().fit_transform(df[col].astype(str))\n",
    "\n",
    "# Handling missing values (fill with mean)\n",
    "df.fillna(df.mean(), inplace=True)\n",
    "\n",
    "# Normalizing the data\n",
    "scaler = StandardScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df), columns=df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Feature Importance using Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf.fit(df_scaled, y)\n",
    "\n",
    "# feature importances\n",
    "feature_importance = pd.Series(rf.feature_importances_, index=df_scaled.columns).sort_values(ascending=False)\n",
    "\n",
    "# Plotting Feature Importance\n",
    "plt.figure(figsize=(12, 12))\n",
    "sns.barplot(x=feature_importance.values, y=feature_importance.index)\n",
    "plt.title(\"Feature Importance (Random Forest)\")\n",
    "plt.xlabel(\"Importance Score\")\n",
    "plt.ylabel(\"Feature\")\n",
    "plt.show()\n",
    "\n",
    "#  Checking the features having low importance (less than 0.005) in contributing to the target variable\n",
    "low_importance_threshold = 0.005\n",
    "low_importance_features = feature_importance[feature_importance < low_importance_threshold].index.tolist()\n",
    "\n",
    "print(\"Features to be dropped due to low importance:\", low_importance_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping features that have very low importance\n",
    "df_selected = df_scaled.drop(columns=low_importance_features)\n",
    "\n",
    "# Recursive Feature Elimination (RFE)\n",
    "rfe = RFECV(estimator=rf, step=1, cv=5, scoring='accuracy')\n",
    "rfe.fit(df_selected, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing correlation matrix\n",
    "corr_matrix = df_selected.corr()\n",
    "\n",
    "# Setting threshold for high correlation\n",
    "threshold = 0.9\n",
    "\n",
    "# Finding highly correlated features\n",
    "high_corr_pairs = set()\n",
    "for i in range(len(corr_matrix.columns)):\n",
    "    for j in range(i):\n",
    "        if abs(corr_matrix.iloc[i, j]) > threshold:\n",
    "            high_corr_pairs.add((corr_matrix.columns[i], corr_matrix.columns[j]))\n",
    "\n",
    "# Extracting features to drop (keeping one from each correlated pair)\n",
    "features_to_drop = set([pair[1] for pair in high_corr_pairs])\n",
    "\n",
    "# Dropping highly correlated features\n",
    "df_final_reduced = df_selected.drop(columns=features_to_drop)\n",
    "\n",
    "#correlation heatmap\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Feature Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "#list of dropped features\n",
    "features_to_drop\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing the selected clinical data\n",
    "scaler = StandardScaler()\n",
    "df_selected_normalized = scaler.fit_transform(df_final_reduced)  # Shape: (105, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to PyTorch tensor\n",
    "patient_data = torch.tensor(df_selected_normalized, dtype=torch.float32)  # Shape: (105, num_features)\n",
    "\n",
    "# Defining PVEM module\n",
    "class PVEM(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim=128):\n",
    "        super(PVEM, self).__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(input_dim, embedding_dim))  # Learnable transformation\n",
    "        self.bias = nn.Parameter(torch.randn(input_dim, embedding_dim))  # Feature-wise bias\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x.unsqueeze(-1) * self.weight + self.bias  # Output shape: (105, num_features, 128)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing PVEM model\n",
    "num_features = patient_data.shape[1]  # Number of selected features\n",
    "embedding_dim = 128\n",
    "model = PVEM(num_features, embedding_dim)\n",
    "\n",
    "# Generating embeddings using PVEM\n",
    "embeddings = model(patient_data).detach().numpy()  # Shape: (105, num_features, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embeddings Generated Successfully!\")\n",
    "print(\"Final Shape:\", embeddings.shape)  # Expected: (105, num_features, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Embedding of first patient (first feature):\\n\", embeddings[0, 0])  # Shape: (128,)\n",
    "print(\"Embedding of first patient (second feature):\\n\", embeddings[0, 1])  # Shape: (128,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "sns.heatmap(embeddings[0], cmap=\"coolwarm\", annot=False)\n",
    "plt.title(\"Heatmap of First Patient's Feature Embeddings\")\n",
    "plt.xlabel(\"Embedding Dimension (128)\")\n",
    "plt.ylabel(\"Feature Index\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten embeddings to (105, num_features * 128)\n",
    "flat_embeddings = embeddings.reshape(105, -1)\n",
    "\n",
    "# Compute similarity matrix\n",
    "similarity_matrix = cosine_similarity(flat_embeddings)\n",
    "\n",
    "# Extract only the first 5 patients' similarities\n",
    "similarity_subset = similarity_matrix[:5, :5]\n",
    "\n",
    "# Plot heatmap\n",
    "plt.figure(figsize=(6, 4))  # Adjust size\n",
    "ax = sns.heatmap(similarity_subset, annot=True, cmap=\"coolwarm\", linewidths=0.5, fmt=\".2f\")\n",
    "\n",
    "# Labels\n",
    "plt.title(\"Cosine Similarity Heatmap (First 5 Patients)\", fontsize=12)\n",
    "plt.xticks(ticks=range(5), labels=[f\"P{i+1}\" for i in range(5)], fontsize=10)\n",
    "plt.yticks(ticks=range(5), labels=[f\"P{i+1}\" for i in range(5)], fontsize=10, rotation=0)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving embeddings as NumPy file\n",
    "np.save(\"patient_feature_embeddings.npy\", embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_data = embeddings.reshape(-1, 128)\n",
    "\n",
    "# Creating a multi-index for (105, 38) patients & features\n",
    "index = pd.MultiIndex.from_product([range(105), range(38)], names=[\"Patient\", \"Feature\"])\n",
    "df = pd.DataFrame(reshaped_data, index=index)\n",
    "\n",
    "# Saving as CSV\n",
    "df.to_csv(\"output_multiindex.csv\")\n",
    "\n",
    "print(\"CSV saved with shape:\", df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
