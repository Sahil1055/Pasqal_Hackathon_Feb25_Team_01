{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch.utils.data import TensorDataset\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, global_mean_pool, GATConv\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim = 128\n",
    "n_clinical = 38 \n",
    "n_image_nodes = 6*6\n",
    "n_nodes = n_clinical + n_image_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples:  84\n",
      "Test Samples:  21\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Ground-Truth Values\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()                 # (n_train,)\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()                   # (n_test,)\n",
    "\n",
    "n_train = len(train_labels) # 84\n",
    "n_test = len(test_labels)   # 21\n",
    "\n",
    "print('Training Samples: ', n_train)\n",
    "print('Test Samples: ', n_test)\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.long)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)                # Should be (n_train,)\n",
    "print(\"Test labels shape:\", test_labels.shape)                  # Should be (n_test,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_embeddings(embeddings):\n",
    "    return (embeddings - embeddings.mean()) / (embeddings.std() + 1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Image Embeddings:  (84, 6, 6, 128)\n",
      "Train Clinical Embeddings:  (84, 38, 128)\n",
      "Test Image Embeddings:  (21, 6, 6, 128)\n",
      "Test Clinical Embeddings:  (21, 38, 128)\n",
      "Train labels shape: torch.Size([84])\n",
      "Test labels shape: torch.Size([21])\n"
     ]
    }
   ],
   "source": [
    "# Load Embeddings with proper shapes\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Load clinical embeddings which are correctly shaped\n",
    "train_clinical_embeddings = np.load(\"data/clinical_data/train_embeddings.npy\")  # (84, 38, 128)\n",
    "test_clinical_embeddings = np.load(\"data/clinical_data/test_embeddings.npy\")    # (21, 38, 128)\n",
    "\n",
    "# Get proper dimensions from clinical data\n",
    "n_train = train_clinical_embeddings.shape[0]  # 84\n",
    "n_test = test_clinical_embeddings.shape[0]    # 21\n",
    "\n",
    "# Create properly shaped image embeddings (if originals are empty)\n",
    "train_image_path = \"data/image_data/train_embeddings.npy\"\n",
    "test_image_path = \"data/image_data/test_embeddings.npy\"\n",
    "\n",
    "# Check if image embeddings exist and have proper shape\n",
    "if os.path.exists(train_image_path) and np.load(train_image_path).size > 0:\n",
    "    train_image_embeddings = np.load(train_image_path)\n",
    "else:\n",
    "    print(\"Warning: Creating placeholder train image embeddings\")\n",
    "    # Create random placeholders with correct shape\n",
    "    train_image_embeddings = np.random.normal(0, 0.1, size=(n_train, 6, 6, embed_dim))\n",
    "    # Save the placeholders for future use\n",
    "    np.save(train_image_path, train_image_embeddings)\n",
    "\n",
    "if os.path.exists(test_image_path) and np.load(test_image_path).size > 0:\n",
    "    test_image_embeddings = np.load(test_image_path)\n",
    "else:\n",
    "    print(\"Warning: Creating placeholder test image embeddings\")\n",
    "    # Create random placeholders with correct shape  \n",
    "    test_image_embeddings = np.random.normal(0, 0.1, size=(n_test, 6, 6, embed_dim))\n",
    "    # Save the placeholders for future use\n",
    "    np.save(test_image_path, test_image_embeddings)\n",
    "\n",
    "print(\"Train Image Embeddings: \", train_image_embeddings.shape)      # Should be (84, 6, 6, 128)\n",
    "print(\"Train Clinical Embeddings: \", train_clinical_embeddings.shape)# Should be (84, 38, 128)\n",
    "print(\"Test Image Embeddings: \", test_image_embeddings.shape)        # Should be (21, 6, 6, 128)\n",
    "print(\"Test Clinical Embeddings: \", test_clinical_embeddings.shape)  # Should be (21, 38, 128)\n",
    "\n",
    "# Also update the labels to match the embedding count\n",
    "# Use clinical embeddings count since that's already correct\n",
    "train_labels = pd.read_csv(\"data/labels/train_labels.csv\")\n",
    "train_labels = train_labels.iloc[:, 1].tolist()[:n_train]  # Ensure we have exactly n_train labels\n",
    "test_labels = pd.read_csv(\"data/labels/test_labels.csv\")\n",
    "test_labels = test_labels.iloc[:, 1].tolist()[:n_test]     # Ensure we have exactly n_test labels\n",
    "\n",
    "# Convert to tensors\n",
    "train_labels = torch.tensor(train_labels, dtype=torch.float)\n",
    "test_labels = torch.tensor(test_labels, dtype=torch.float)\n",
    "\n",
    "print(\"Train labels shape:\", train_labels.shape)  # Should be (84,)\n",
    "print(\"Test labels shape:\", test_labels.shape)    # Should be (21,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reshaped Train Image Embeddings:  torch.Size([84, 36, 128])\n",
      "Combined Train Embeddings:  torch.Size([84, 74, 128])\n",
      "Reshaped Test Image Embeddings:  torch.Size([21, 36, 128])\n",
      "Combined Test Embeddings:  torch.Size([21, 74, 128])\n"
     ]
    }
   ],
   "source": [
    "# Reshape image embeddings to match size of clinical embeddings\n",
    "train_image_features = torch.tensor(train_image_embeddings.reshape(n_train, 36, embed_dim))                             # Shape: [n_train, 36, embed_dim]\n",
    "test_image_features = torch.tensor(test_image_embeddings.reshape(n_test, 36, embed_dim))                                # Shape: [n_test, 36, embed_dim]\n",
    "\n",
    "# Combine clinical and image features\n",
    "train_patient_features = torch.cat([torch.tensor(train_clinical_embeddings), train_image_features], dim=1)              # Shape: [n_train, 74, embed_dim]\n",
    "test_patient_features = torch.cat([torch.tensor(test_clinical_embeddings), test_image_features], dim=1)                 # Shape: [n_test, 74, embed_dim]\n",
    "\n",
    "print('Reshaped Train Image Embeddings: ', train_image_features.shape)\n",
    "print('Combined Train Embeddings: ', train_patient_features.shape)\n",
    "print('Reshaped Test Image Embeddings: ', test_image_features.shape)\n",
    "print('Combined Test Embeddings: ', test_patient_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_patient_edges(n_clinical, n_nodes):\n",
    "    \"\"\"\n",
    "    Creates bidirectional edges between clinical nodes and image nodes.\n",
    "    Adds a self-edge to each node.\n",
    "\n",
    "    Total edges = n_nodes (self-edges) + 2 * n_clinical * n_image_nodes (bidirectional edges)\n",
    "\n",
    "    Parameters:\n",
    "    - n_clinical: number of clinical nodes (for a specific patient)\n",
    "    - n_image_nodes: number of image nodes (for a specific patient)\n",
    "    \"\"\"\n",
    "    node_ids = np.expand_dims(np.arange(n_nodes, dtype=int), 0)\n",
    "    # self-edges = preserves some features of each own node during a graph convolution\n",
    "    self_edges = np.concatenate((node_ids, node_ids), 0)\n",
    "\n",
    "    # clinical nodes\n",
    "    c_array_asc = np.expand_dims(np.arange(n_clinical), 0)\n",
    "    all_edges = self_edges[:]\n",
    "\n",
    "    for i in range(n_clinical, n_nodes):\n",
    "        # image nodes\n",
    "        i_array = np.expand_dims(np.array([i]*n_clinical), 0)\n",
    "\n",
    "        # image --> clinical\n",
    "        inter_edges_ic = np.concatenate((i_array, c_array_asc), 0)\n",
    "        # clinical --> image\n",
    "        inter_edges_ci = np.concatenate((c_array_asc, i_array), 0)\n",
    "\n",
    "        # bidirectional edges\n",
    "        inter_edges_i = np.concatenate((inter_edges_ic, inter_edges_ci), 1)\n",
    "        all_edges = np.concatenate((all_edges, inter_edges_i), 1)\n",
    "\n",
    "    return torch.tensor(all_edges, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_list(patient_features, patient_labels):\n",
    "    \"\"\"\n",
    "    Generates a sub-graph for each patient given its embeddings\n",
    "\n",
    "    Parameters:\n",
    "    - patient_features: combined clinical and image embeddings of one patient\n",
    "    - patient_labels: groud truth values\n",
    "    \"\"\"\n",
    "    data_list = []\n",
    "    for i in range(len(patient_labels)):\n",
    "        # Create the graph for each patient\n",
    "        patient_edges = create_patient_edges(n_clinical, n_nodes)   # Shape: [2, num_edges]\n",
    "        patient_y = patient_labels[i]                               # Target label for this patient\n",
    "\n",
    "        data = Data(x=patient_features[i], edge_index=patient_edges, y=patient_y)\n",
    "        data_list.append(data)\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Patients:  84\n",
      "Test Patients:  21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    }
   ],
   "source": [
    "train_data_list = get_data_list(train_patient_features, train_labels)\n",
    "test_data_list = get_data_list(test_patient_features, test_labels)\n",
    "\n",
    "# Batch size 1 for individual patients\n",
    "train_loader = DataLoader(train_data_list, batch_size=1, shuffle=False, num_workers=0)  \n",
    "test_loader = DataLoader(test_data_list, batch_size=1, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"Train Patients: \", len(train_loader))\n",
    "print(\"Test Patients: \", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "We define the Graph Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, dropout=0.5):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels)          # Second GCN layer\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)                   # Fully connected layer for binary classification\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        # Apply graph convolution\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        # Global pooling (mean) across all nodes\n",
    "        x = global_mean_pool(x, batch)  # This will aggregate node features into one scalar per graph\n",
    "        \n",
    "        # Pass the aggregated feature through a fully connected layer to get a single logit\n",
    "        x = self.fc(x)  # Output size is (batch_size, 1)\n",
    "        return x  # Output a single logit for each patient (before applying sigmoid in loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Graph Attention Network\n",
    "class GAT(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, heads=2, dropout=0.5):\n",
    "        super(GAT, self).__init__()\n",
    "        self.conv1 = GATConv(in_channels, hidden_channels, heads=heads, dropout=dropout)\n",
    "        self.conv2 = GATConv(hidden_channels * heads, hidden_channels, heads=1, dropout=dropout)\n",
    "        self.fc = torch.nn.Linear(hidden_channels, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "    \n",
    "    def forward(self, x, edge_index, batch):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)          # Aggregate node features\n",
    "        x = self.fc(x)                          # Binary classification output\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# Model Parameters\n",
    "learning_rate = 0.0001\n",
    "w_decay = 5e-4\n",
    "hidden_channels = 128\n",
    "\n",
    "# Initialize Model\n",
    "model = GCN(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "# model = GAT(in_channels=embed_dim, hidden_channels=hidden_channels)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/300, Loss: 0.6912061181806383\n",
      "Epoch 2/300, Loss: 0.6832071862050465\n",
      "Epoch 3/300, Loss: 0.6767535003877821\n",
      "Epoch 4/300, Loss: 0.670995491601172\n",
      "Epoch 5/300, Loss: 0.6656828040168399\n",
      "Epoch 6/300, Loss: 0.6604420297912189\n",
      "Epoch 7/300, Loss: 0.6550281611936433\n",
      "Epoch 8/300, Loss: 0.6492231442105203\n",
      "Epoch 9/300, Loss: 0.6428188521947179\n",
      "Epoch 10/300, Loss: 0.6356555227012861\n",
      "Epoch 11/300, Loss: 0.6275590094072478\n",
      "Epoch 12/300, Loss: 0.6184864232227916\n",
      "Epoch 13/300, Loss: 0.608458929118656\n",
      "Epoch 14/300, Loss: 0.5975445560401395\n",
      "Epoch 15/300, Loss: 0.5859113174180189\n",
      "Epoch 16/300, Loss: 0.5736425777985936\n",
      "Epoch 17/300, Loss: 0.5608044800659021\n",
      "Epoch 18/300, Loss: 0.5475905817002058\n",
      "Epoch 19/300, Loss: 0.5339563997196299\n",
      "Epoch 20/300, Loss: 0.52019685258468\n",
      "Epoch 21/300, Loss: 0.506422410231261\n",
      "Epoch 22/300, Loss: 0.4927857428168257\n",
      "Epoch 23/300, Loss: 0.47920084624950376\n",
      "Epoch 24/300, Loss: 0.4658464625300396\n",
      "Epoch 25/300, Loss: 0.45263446987207445\n",
      "Epoch 26/300, Loss: 0.4395995861185448\n",
      "Epoch 27/300, Loss: 0.4267347914920676\n",
      "Epoch 28/300, Loss: 0.4140010040829934\n",
      "Epoch 29/300, Loss: 0.4014803943822959\n",
      "Epoch 30/300, Loss: 0.38923098682425916\n",
      "Epoch 31/300, Loss: 0.37697382939846386\n",
      "Epoch 32/300, Loss: 0.36500807993468787\n",
      "Epoch 33/300, Loss: 0.3532138121635875\n",
      "Epoch 34/300, Loss: 0.34152190585709397\n",
      "Epoch 35/300, Loss: 0.33014745554066305\n",
      "Epoch 36/300, Loss: 0.3188863632302465\n",
      "Epoch 37/300, Loss: 0.30772906414876205\n",
      "Epoch 38/300, Loss: 0.29682529306349653\n",
      "Epoch 39/300, Loss: 0.2860797808638641\n",
      "Epoch 40/300, Loss: 0.27551366352487267\n",
      "Epoch 41/300, Loss: 0.2652081867910012\n",
      "Epoch 42/300, Loss: 0.25500565768396927\n",
      "Epoch 43/300, Loss: 0.24504714059903995\n",
      "Epoch 44/300, Loss: 0.23529736344200847\n",
      "Epoch 45/300, Loss: 0.22558261046443867\n",
      "Epoch 46/300, Loss: 0.21630757368984632\n",
      "Epoch 47/300, Loss: 0.20694403618628193\n",
      "Epoch 48/300, Loss: 0.19820548069695915\n",
      "Epoch 49/300, Loss: 0.189288862587654\n",
      "Epoch 50/300, Loss: 0.1808399305257245\n",
      "Epoch 51/300, Loss: 0.17264177901303074\n",
      "Epoch 52/300, Loss: 0.16457273726181093\n",
      "Epoch 53/300, Loss: 0.15665328883300217\n",
      "Epoch 54/300, Loss: 0.14913044398136185\n",
      "Epoch 55/300, Loss: 0.1417846035210676\n",
      "Epoch 56/300, Loss: 0.1347983957611437\n",
      "Epoch 57/300, Loss: 0.12802180736948035\n",
      "Epoch 58/300, Loss: 0.12139526435098127\n",
      "Epoch 59/300, Loss: 0.11517830752474029\n",
      "Epoch 60/300, Loss: 0.10908608121450297\n",
      "Epoch 61/300, Loss: 0.10323106381229406\n",
      "Epoch 62/300, Loss: 0.09764810997341469\n",
      "Epoch 63/300, Loss: 0.09227789176406445\n",
      "Epoch 64/300, Loss: 0.08711516089059453\n",
      "Epoch 65/300, Loss: 0.08228784865798795\n",
      "Epoch 66/300, Loss: 0.07752848947037619\n",
      "Epoch 67/300, Loss: 0.0731468003802853\n",
      "Epoch 68/300, Loss: 0.06885206416078152\n",
      "Epoch 69/300, Loss: 0.06485443201476317\n",
      "Epoch 70/300, Loss: 0.0610326920751833\n",
      "Epoch 71/300, Loss: 0.05752769115454827\n",
      "Epoch 72/300, Loss: 0.054096651977488024\n",
      "Epoch 73/300, Loss: 0.05097769795186945\n",
      "Epoch 74/300, Loss: 0.0479561935804935\n",
      "Epoch 75/300, Loss: 0.045191838363499284\n",
      "Epoch 76/300, Loss: 0.04252331542372993\n",
      "Epoch 77/300, Loss: 0.040095739731157126\n",
      "Epoch 78/300, Loss: 0.0377350425054256\n",
      "Epoch 79/300, Loss: 0.03562023644491454\n",
      "Epoch 80/300, Loss: 0.033548359476211785\n",
      "Epoch 81/300, Loss: 0.031689846989960996\n",
      "Epoch 82/300, Loss: 0.029891173573364852\n",
      "Epoch 83/300, Loss: 0.02826773115490739\n",
      "Epoch 84/300, Loss: 0.026696724231412346\n",
      "Epoch 85/300, Loss: 0.025277962797645076\n",
      "Epoch 86/300, Loss: 0.02390745544004492\n",
      "Epoch 87/300, Loss: 0.022663603461751253\n",
      "Epoch 88/300, Loss: 0.02148355869525438\n",
      "Epoch 89/300, Loss: 0.020384743502412612\n",
      "Epoch 90/300, Loss: 0.019350775858505302\n",
      "Epoch 91/300, Loss: 0.018396249887642636\n",
      "Epoch 92/300, Loss: 0.017496431821621444\n",
      "Epoch 93/300, Loss: 0.016661309949008846\n",
      "Epoch 94/300, Loss: 0.015880809948734104\n",
      "Epoch 95/300, Loss: 0.015149337860251643\n",
      "Epoch 96/300, Loss: 0.014468320046068768\n",
      "Epoch 97/300, Loss: 0.013828865711776047\n",
      "Epoch 98/300, Loss: 0.013236494112994652\n",
      "Epoch 99/300, Loss: 0.012673840159197067\n",
      "Epoch 100/300, Loss: 0.012151550685370608\n",
      "Epoch 101/300, Loss: 0.011662025100329336\n",
      "Epoch 102/300, Loss: 0.011205597825923383\n",
      "Epoch 103/300, Loss: 0.010771371414962448\n",
      "Epoch 104/300, Loss: 0.010366043981480829\n",
      "Epoch 105/300, Loss: 0.009988443240698211\n",
      "Epoch 106/300, Loss: 0.009633058427260451\n",
      "Epoch 107/300, Loss: 0.00930081417293612\n",
      "Epoch 108/300, Loss: 0.00898725224036498\n",
      "Epoch 109/300, Loss: 0.008695725542388627\n",
      "Epoch 110/300, Loss: 0.00842070886380614\n",
      "Epoch 111/300, Loss: 0.008163344232195462\n",
      "Epoch 112/300, Loss: 0.007920983212552338\n",
      "Epoch 113/300, Loss: 0.007694511305440902\n",
      "Epoch 114/300, Loss: 0.007479485507248894\n",
      "Epoch 115/300, Loss: 0.007280242696881244\n",
      "Epoch 116/300, Loss: 0.007091537434163797\n",
      "Epoch 117/300, Loss: 0.006915477768801213\n",
      "Epoch 118/300, Loss: 0.00674837107440445\n",
      "Epoch 119/300, Loss: 0.006592158868246299\n",
      "Epoch 120/300, Loss: 0.00644490897015192\n",
      "Epoch 121/300, Loss: 0.006304325263283231\n",
      "Epoch 122/300, Loss: 0.006173301736796908\n",
      "Epoch 123/300, Loss: 0.006048175231959314\n",
      "Epoch 124/300, Loss: 0.005932040725942518\n",
      "Epoch 125/300, Loss: 0.005821333120468218\n",
      "Epoch 126/300, Loss: 0.005716372187655966\n",
      "Epoch 127/300, Loss: 0.00561694591068964\n",
      "Epoch 128/300, Loss: 0.005525347444538349\n",
      "Epoch 129/300, Loss: 0.0054352519061015275\n",
      "Epoch 130/300, Loss: 0.005352266292294958\n",
      "Epoch 131/300, Loss: 0.0052709176306921434\n",
      "Epoch 132/300, Loss: 0.005195968904736165\n",
      "Epoch 133/300, Loss: 0.005121876992929299\n",
      "Epoch 134/300, Loss: 0.005054222498072493\n",
      "Epoch 135/300, Loss: 0.004987030215632769\n",
      "Epoch 136/300, Loss: 0.004924209000754959\n",
      "Epoch 137/300, Loss: 0.004862267008567485\n",
      "Epoch 138/300, Loss: 0.004802861596775729\n",
      "Epoch 139/300, Loss: 0.004745951786586845\n",
      "Epoch 140/300, Loss: 0.0046915479608491935\n",
      "Epoch 141/300, Loss: 0.0046371482105518865\n",
      "Epoch 142/300, Loss: 0.0045867933757283\n",
      "Epoch 143/300, Loss: 0.004535943919469517\n",
      "Epoch 144/300, Loss: 0.004489438069430072\n",
      "Epoch 145/300, Loss: 0.0044407707648283735\n",
      "Epoch 146/300, Loss: 0.004396380119722548\n",
      "Epoch 147/300, Loss: 0.004350907029999142\n",
      "Epoch 148/300, Loss: 0.004307830360199698\n",
      "Epoch 149/300, Loss: 0.004263362000225172\n",
      "Epoch 150/300, Loss: 0.004222171425153999\n",
      "Epoch 151/300, Loss: 0.0041796885751562655\n",
      "Epoch 152/300, Loss: 0.004140476789569023\n",
      "Epoch 153/300, Loss: 0.004098977608768518\n",
      "Epoch 154/300, Loss: 0.0040623882423348944\n",
      "Epoch 155/300, Loss: 0.004023871142846265\n",
      "Epoch 156/300, Loss: 0.003987354464267277\n",
      "Epoch 157/300, Loss: 0.003950935569077292\n",
      "Epoch 158/300, Loss: 0.0039162373654878565\n",
      "Epoch 159/300, Loss: 0.0038820507669439864\n",
      "Epoch 160/300, Loss: 0.0038501860134750082\n",
      "Epoch 161/300, Loss: 0.003815656344250618\n",
      "Epoch 162/300, Loss: 0.0037829328022988577\n",
      "Epoch 163/300, Loss: 0.00375227577476184\n",
      "Epoch 164/300, Loss: 0.003723191235556623\n",
      "Epoch 165/300, Loss: 0.0036919130838822547\n",
      "Epoch 166/300, Loss: 0.003662794137788694\n",
      "Epoch 167/300, Loss: 0.0036340201649979675\n",
      "Epoch 168/300, Loss: 0.003606239481909497\n",
      "Epoch 169/300, Loss: 0.003579851708495867\n",
      "Epoch 170/300, Loss: 0.0035537793248178132\n",
      "Epoch 171/300, Loss: 0.0035273524850093657\n",
      "Epoch 172/300, Loss: 0.0035025313272471622\n",
      "Epoch 173/300, Loss: 0.0034794841618573883\n",
      "Epoch 174/300, Loss: 0.0034562197589915215\n",
      "Epoch 175/300, Loss: 0.003433430942030281\n",
      "Epoch 176/300, Loss: 0.0034111330240493417\n",
      "Epoch 177/300, Loss: 0.003391351947822416\n",
      "Epoch 178/300, Loss: 0.003371880934818502\n",
      "Epoch 179/300, Loss: 0.0033528428796023896\n",
      "Epoch 180/300, Loss: 0.0033336265342424013\n",
      "Epoch 181/300, Loss: 0.0033152702465325695\n",
      "Epoch 182/300, Loss: 0.0032982965524537703\n",
      "Epoch 183/300, Loss: 0.003281469355599481\n",
      "Epoch 184/300, Loss: 0.0032641960291733036\n",
      "Epoch 185/300, Loss: 0.0032476116082845253\n",
      "Epoch 186/300, Loss: 0.003231806505828518\n",
      "Epoch 187/300, Loss: 0.0032169493407695746\n",
      "Epoch 188/300, Loss: 0.003201536017518793\n",
      "Epoch 189/300, Loss: 0.00318852579566487\n",
      "Epoch 190/300, Loss: 0.003174918048297435\n",
      "Epoch 191/300, Loss: 0.0031604823581007834\n",
      "Epoch 192/300, Loss: 0.0031487198865747246\n",
      "Epoch 193/300, Loss: 0.0031371415417908786\n",
      "Epoch 194/300, Loss: 0.0031251568043495176\n",
      "Epoch 195/300, Loss: 0.00311468712679675\n",
      "Epoch 196/300, Loss: 0.0031041800775869753\n",
      "Epoch 197/300, Loss: 0.0030941106084417433\n",
      "Epoch 198/300, Loss: 0.003084293951728241\n",
      "Epoch 199/300, Loss: 0.0030766588183070774\n",
      "Epoch 200/300, Loss: 0.0030690200868003594\n",
      "Epoch 201/300, Loss: 0.003059374161038604\n",
      "Epoch 202/300, Loss: 0.003051723955974713\n",
      "Epoch 203/300, Loss: 0.0030432866344813057\n",
      "Epoch 204/300, Loss: 0.0030362925219775895\n",
      "Epoch 205/300, Loss: 0.003030224956724564\n",
      "Epoch 206/300, Loss: 0.0030226025752095134\n",
      "Epoch 207/300, Loss: 0.0030174219235725615\n",
      "Epoch 208/300, Loss: 0.0030094174672419577\n",
      "Epoch 209/300, Loss: 0.003003544126824089\n",
      "Epoch 210/300, Loss: 0.002998382995054764\n",
      "Epoch 211/300, Loss: 0.00299440601689853\n",
      "Epoch 212/300, Loss: 0.0029868571853538264\n",
      "Epoch 213/300, Loss: 0.002981734172954426\n",
      "Epoch 214/300, Loss: 0.002977448444955907\n",
      "Epoch 215/300, Loss: 0.0029720001268411047\n",
      "Epoch 216/300, Loss: 0.002967950102536693\n",
      "Epoch 217/300, Loss: 0.0029640725873490295\n",
      "Epoch 218/300, Loss: 0.002960049014707319\n",
      "Epoch 219/300, Loss: 0.0029574092197071607\n",
      "Epoch 220/300, Loss: 0.002952794108544427\n",
      "Epoch 221/300, Loss: 0.002949781062743758\n",
      "Epoch 222/300, Loss: 0.0029451773629406695\n",
      "Epoch 223/300, Loss: 0.0029432225651463894\n",
      "Epoch 224/300, Loss: 0.0029413877494577144\n",
      "Epoch 225/300, Loss: 0.0029379728201335063\n",
      "Epoch 226/300, Loss: 0.00293352326468112\n",
      "Epoch 227/300, Loss: 0.0029308541281475677\n",
      "Epoch 228/300, Loss: 0.0029291682315892957\n",
      "Epoch 229/300, Loss: 0.0029261411023064794\n",
      "Epoch 230/300, Loss: 0.002924315029904365\n",
      "Epoch 231/300, Loss: 0.00292310133917321\n",
      "Epoch 232/300, Loss: 0.0029204417088623457\n",
      "Epoch 233/300, Loss: 0.0029181936405170256\n",
      "Epoch 234/300, Loss: 0.002916741665265206\n",
      "Epoch 235/300, Loss: 0.0029151790727690056\n",
      "Epoch 236/300, Loss: 0.00291338785364931\n",
      "Epoch 237/300, Loss: 0.0029110630414751746\n",
      "Epoch 238/300, Loss: 0.002909062742244963\n",
      "Epoch 239/300, Loss: 0.0029099866344644114\n",
      "Epoch 240/300, Loss: 0.002906060160164597\n",
      "Epoch 241/300, Loss: 0.002905831493934773\n",
      "Epoch 242/300, Loss: 0.0029044463650656042\n",
      "Epoch 243/300, Loss: 0.0029015373041978536\n",
      "Epoch 244/300, Loss: 0.0029002316361592356\n",
      "Epoch 245/300, Loss: 0.0029000450276874916\n",
      "Epoch 246/300, Loss: 0.0028989393938161356\n",
      "Epoch 247/300, Loss: 0.0028968706931574877\n",
      "Epoch 248/300, Loss: 0.002893283498672244\n",
      "Epoch 249/300, Loss: 0.002893077055707257\n",
      "Epoch 250/300, Loss: 0.0028920198103862945\n",
      "Epoch 251/300, Loss: 0.002890021857642263\n",
      "Epoch 252/300, Loss: 0.002888729596318875\n",
      "Epoch 253/300, Loss: 0.0028886850679779876\n",
      "Epoch 254/300, Loss: 0.0028860570752415457\n",
      "Epoch 255/300, Loss: 0.0028847449493249705\n",
      "Epoch 256/300, Loss: 0.002884208129854317\n",
      "Epoch 257/300, Loss: 0.002883116358154731\n",
      "Epoch 258/300, Loss: 0.002881973670418975\n",
      "Epoch 259/300, Loss: 0.002880808657208501\n",
      "Epoch 260/300, Loss: 0.0028797386507998604\n",
      "Epoch 261/300, Loss: 0.00287966628380061\n",
      "Epoch 262/300, Loss: 0.002876931707834815\n",
      "Epoch 263/300, Loss: 0.0028770578338200687\n",
      "Epoch 264/300, Loss: 0.002875469367823701\n",
      "Epoch 265/300, Loss: 0.0028745303415799333\n",
      "Epoch 266/300, Loss: 0.0028737946135283295\n",
      "Epoch 267/300, Loss: 0.002872607477411428\n",
      "Epoch 268/300, Loss: 0.002872517038059506\n",
      "Epoch 269/300, Loss: 0.0028697934454007573\n",
      "Epoch 270/300, Loss: 0.002869623409790192\n",
      "Epoch 271/300, Loss: 0.0028713766662112525\n",
      "Epoch 272/300, Loss: 0.0028703049271394136\n",
      "Epoch 273/300, Loss: 0.002867617820657962\n",
      "Epoch 274/300, Loss: 0.0028657784021951676\n",
      "Epoch 275/300, Loss: 0.002866081484115194\n",
      "Epoch 276/300, Loss: 0.002866341533412507\n",
      "Epoch 277/300, Loss: 0.002865853236404515\n",
      "Epoch 278/300, Loss: 0.0028640958692514673\n",
      "Epoch 279/300, Loss: 0.002861419468356359\n",
      "Epoch 280/300, Loss: 0.002862598660372687\n",
      "Epoch 281/300, Loss: 0.0028624851644422343\n",
      "Epoch 282/300, Loss: 0.002860779914291456\n",
      "Epoch 283/300, Loss: 0.002861060126198467\n",
      "Epoch 284/300, Loss: 0.0028607190193985182\n",
      "Epoch 285/300, Loss: 0.0028596781310257753\n",
      "Epoch 286/300, Loss: 0.0028574456761708924\n",
      "Epoch 287/300, Loss: 0.0028564097217311157\n",
      "Epoch 288/300, Loss: 0.002858418960968749\n",
      "Epoch 289/300, Loss: 0.002856924545938592\n",
      "Epoch 290/300, Loss: 0.002856080155671171\n",
      "Epoch 291/300, Loss: 0.0028530058844628585\n",
      "Epoch 292/300, Loss: 0.0028546808296546714\n",
      "Epoch 293/300, Loss: 0.0028535263955782953\n",
      "Epoch 294/300, Loss: 0.002851686471187503\n",
      "Epoch 295/300, Loss: 0.0028503851041168475\n",
      "Epoch 296/300, Loss: 0.002849463962295996\n",
      "Epoch 297/300, Loss: 0.00285074233322621\n",
      "Epoch 298/300, Loss: 0.0028500853622619765\n",
      "Epoch 299/300, Loss: 0.002848485787318365\n",
      "Epoch 300/300, Loss: 0.0028459645014159377\n"
     ]
    }
   ],
   "source": [
    "# TRAINING\n",
    "train_losses = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "epochs = 300\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    for data in train_loader:                                               # Iterate over each batch (here, each batch is one patient)\n",
    "                                                                            # Data object contains 'x' (features), 'edge_index' (graph edges), 'y' (labels)\n",
    "        patient_features = data.x                                           # Shape: (num_nodes, in_channels)\n",
    "        patient_edges = data.edge_index                                     # Shape: (2, num_edges)\n",
    "        patient_label = data.y.float()                                      # Target label\n",
    "        batch = data.batch\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()\n",
    "        patient_edges = patient_edges.to(torch.long)                 \n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        output = model(patient_features, patient_edges, batch)                  # Output shape: (1, 1)\n",
    "        \n",
    "        # Binary Classification Loss\n",
    "        loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), patient_label)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Calculate average loss for this epoch\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    \n",
    "    train_losses.append(avg_loss)\n",
    "\n",
    "    # Print loss after each epoch\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loss Plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(1, epochs + 1), train_losses, label='Training Loss', color='blue')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Epochs')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 71.42857142857143%\n",
      "Precision: 0.6153846153846154\n",
      "Recall: 0.8888888888888888\n",
      "F1-Score: 0.7272727272727273\n"
     ]
    }
   ],
   "source": [
    "# TESTING\n",
    "model.eval() \n",
    "\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data in test_loader:                            # Iterate over each batch (i.e. one patient)\n",
    "        patient_features = data.x                       # Get features (shape: [num_nodes, in_channels])\n",
    "        patient_edges = data.edge_index                 # Get edges (shape: [2, num_edges])\n",
    "        patient_label = data.y.float()                  # Get label (shape: [1])\n",
    "\n",
    "        # Ensure correct format\n",
    "        patient_features = patient_features.float()    \n",
    "        patient_edges = patient_edges.to(torch.long)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(patient_features, patient_edges, data.batch)  # Use the batch info to aggregate across nodes\n",
    "\n",
    "        # Apply sigmoid to the output logits and get the predicted class (0 or 1)\n",
    "        pred = torch.sigmoid(output.squeeze())\n",
    "        predicted_class = (pred >= 0.5).float()                     # Threshold at 0.5 to classify as 0 or 1\n",
    "        \n",
    "        # Collect the labels and predictions for metrics\n",
    "        all_labels.append(patient_label.cpu().numpy())\n",
    "        all_predictions.append(predicted_class.cpu().numpy())\n",
    "\n",
    "        # Count correct predictions\n",
    "        correct += (predicted_class == patient_label).sum().item()\n",
    "        total += patient_label.size(0)  # Increment by the number of samples in this batch\n",
    "\n",
    "# Accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "# Calculate Metrics\n",
    "precision = precision_score(all_labels, all_predictions)\n",
    "recall = recall_score(all_labels, all_predictions)\n",
    "f1 = f1_score(all_labels, all_predictions)\n",
    "roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "print(f\"Precision: {precision}\")\n",
    "print(f\"Recall: {recall}\")\n",
    "print(f\"F1-Score: {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiments\n",
    "Test classification with clinical and image embeddings only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim=128, dropout=0.5):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, 1)\n",
    "        self.dropout = torch.nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)             # Binary classification\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Clinical-Only Model\n",
      "Train Features:  torch.Size([84, 4864])\n",
      "Test Features:  torch.Size([21, 4864])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.8014376720502263\n",
      "Epoch 2/100, Loss: 0.7281884278747297\n",
      "Epoch 3/100, Loss: 0.6162609041535428\n",
      "Epoch 4/100, Loss: 0.6223313288674468\n",
      "Epoch 5/100, Loss: 0.6017956584692001\n",
      "Epoch 6/100, Loss: 0.6325822452005619\n",
      "Epoch 7/100, Loss: 0.503720065488735\n",
      "Epoch 8/100, Loss: 0.5169264074387944\n",
      "Epoch 9/100, Loss: 0.5192441181380196\n",
      "Epoch 10/100, Loss: 0.538801854526225\n",
      "Epoch 11/100, Loss: 0.5101282863428683\n",
      "Epoch 12/100, Loss: 0.5285119785528097\n",
      "Epoch 13/100, Loss: 0.48135380404086653\n",
      "Epoch 14/100, Loss: 0.42032575907762204\n",
      "Epoch 15/100, Loss: 0.4324665805295462\n",
      "Epoch 16/100, Loss: 0.42884032079321815\n",
      "Epoch 17/100, Loss: 0.3537601176732486\n",
      "Epoch 18/100, Loss: 0.43218649479659427\n",
      "Epoch 19/100, Loss: 0.3732460112376083\n",
      "Epoch 20/100, Loss: 0.3934940798879844\n",
      "Epoch 21/100, Loss: 0.29508530954869155\n",
      "Epoch 22/100, Loss: 0.35038516664904557\n",
      "Epoch 23/100, Loss: 0.34043467520747245\n",
      "Epoch 24/100, Loss: 0.3102366478214576\n",
      "Epoch 25/100, Loss: 0.3286075343212793\n",
      "Epoch 26/100, Loss: 0.3360646044422451\n",
      "Epoch 27/100, Loss: 0.27323959769758704\n",
      "Epoch 28/100, Loss: 0.31566066778473856\n",
      "Epoch 29/100, Loss: 0.25388933120692464\n",
      "Epoch 30/100, Loss: 0.2747540716293207\n",
      "Epoch 31/100, Loss: 0.24995164192421745\n",
      "Epoch 32/100, Loss: 0.2771061597297722\n",
      "Epoch 33/100, Loss: 0.24417905136888726\n",
      "Epoch 34/100, Loss: 0.24066195944297825\n",
      "Epoch 35/100, Loss: 0.25202588684869565\n",
      "Epoch 36/100, Loss: 0.1998872963931284\n",
      "Epoch 37/100, Loss: 0.21554039844808415\n",
      "Epoch 38/100, Loss: 0.21841870984098324\n",
      "Epoch 39/100, Loss: 0.19033394023143674\n",
      "Epoch 40/100, Loss: 0.18890432169380997\n",
      "Epoch 41/100, Loss: 0.15324828195387108\n",
      "Epoch 42/100, Loss: 0.1573171009447182\n",
      "Epoch 43/100, Loss: 0.18393640223966723\n",
      "Epoch 44/100, Loss: 0.19496480822106002\n",
      "Epoch 45/100, Loss: 0.20540602870697727\n",
      "Epoch 46/100, Loss: 0.17211733939223536\n",
      "Epoch 47/100, Loss: 0.22623232442889826\n",
      "Epoch 48/100, Loss: 0.16015350947277998\n",
      "Epoch 49/100, Loss: 0.14576310363156406\n",
      "Epoch 50/100, Loss: 0.16022111922581767\n",
      "Epoch 51/100, Loss: 0.19452975650910564\n",
      "Epoch 52/100, Loss: 0.13294719967757238\n",
      "Epoch 53/100, Loss: 0.13745674668250613\n",
      "Epoch 54/100, Loss: 0.19522657017755612\n",
      "Epoch 55/100, Loss: 0.13267058766222053\n",
      "Epoch 56/100, Loss: 0.1445517144559884\n",
      "Epoch 57/100, Loss: 0.12931636842802907\n",
      "Epoch 58/100, Loss: 0.15397973804961057\n",
      "Epoch 59/100, Loss: 0.11104757159988099\n",
      "Epoch 60/100, Loss: 0.11515959923566957\n",
      "Epoch 61/100, Loss: 0.09548242539177836\n",
      "Epoch 62/100, Loss: 0.160850926860186\n",
      "Epoch 63/100, Loss: 0.10469131278742541\n",
      "Epoch 64/100, Loss: 0.09338330661345082\n",
      "Epoch 65/100, Loss: 0.08898265727696598\n",
      "Epoch 66/100, Loss: 0.12272953458482118\n",
      "Epoch 67/100, Loss: 0.10152597290329353\n",
      "Epoch 68/100, Loss: 0.06394113417171388\n",
      "Epoch 69/100, Loss: 0.07309632193341117\n",
      "Epoch 70/100, Loss: 0.07152112552860028\n",
      "Epoch 71/100, Loss: 0.05265658133072022\n",
      "Epoch 72/100, Loss: 0.07688760509012567\n",
      "Epoch 73/100, Loss: 0.09004018284723056\n",
      "Epoch 74/100, Loss: 0.061422313064240154\n",
      "Epoch 75/100, Loss: 0.10930755247126388\n",
      "Epoch 76/100, Loss: 0.06896321764377807\n",
      "Epoch 77/100, Loss: 0.10366381817163915\n",
      "Epoch 78/100, Loss: 0.09175997001503189\n",
      "Epoch 79/100, Loss: 0.09784613186019568\n",
      "Epoch 80/100, Loss: 0.07989149201864282\n",
      "Epoch 81/100, Loss: 0.07070643332688557\n",
      "Epoch 82/100, Loss: 0.07352139106045354\n",
      "Epoch 83/100, Loss: 0.07257953724651552\n",
      "Epoch 84/100, Loss: 0.07943258354067163\n",
      "Epoch 85/100, Loss: 0.07663884749492429\n",
      "Epoch 86/100, Loss: 0.10869193233003072\n",
      "Epoch 87/100, Loss: 0.03689382778525404\n",
      "Epoch 88/100, Loss: 0.04924418439162072\n",
      "Epoch 89/100, Loss: 0.05083939200339231\n",
      "Epoch 90/100, Loss: 0.06404855124894618\n",
      "Epoch 91/100, Loss: 0.08531259612975413\n",
      "Epoch 92/100, Loss: 0.06576899186886254\n",
      "Epoch 93/100, Loss: 0.0742500197579498\n",
      "Epoch 94/100, Loss: 0.05434445367882823\n",
      "Epoch 95/100, Loss: 0.047305650016880835\n",
      "Epoch 96/100, Loss: 0.06734392242639078\n",
      "Epoch 97/100, Loss: 0.05397804438420754\n",
      "Epoch 98/100, Loss: 0.05841375120845561\n",
      "Epoch 99/100, Loss: 0.04266362477481101\n",
      "Epoch 100/100, Loss: 0.0262524265284027\n",
      "Clinical-Only Model - Precision: 0.7000, Recall: 0.7778, F1-Score: 0.7368\n",
      "Test Accuracy: 76.19047619047619%\n",
      "Precision: 0.7\n",
      "Recall: 0.7777777777777778\n",
      "F1-Score: 0.7368421052631579\n",
      "\n",
      "Training Image-Only Model\n",
      "Train Features:  torch.Size([84, 4608])\n",
      "Test Features:  torch.Size([21, 4608])\n",
      "Train Labels:  torch.Size([84, 1])\n",
      "Test Labels:  torch.Size([21, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_53284\\1274329738.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
      "C:\\Users\\Usama.Khatab\\AppData\\Local\\Temp\\ipykernel_53284\\1274329738.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
      "c:\\Users\\Usama.Khatab\\Projects\\miniconda3\\envs\\hackathon\\lib\\site-packages\\torch_geometric\\deprecation.py:26: UserWarning: 'data.DataLoader' is deprecated, use 'loader.DataLoader' instead\n",
      "  warnings.warn(out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Loss: 0.6866854166700727\n",
      "Epoch 2/100, Loss: 0.639976177187193\n",
      "Epoch 3/100, Loss: 0.5967240376131875\n",
      "Epoch 4/100, Loss: 0.5475654712035543\n",
      "Epoch 5/100, Loss: 0.5062770669658979\n",
      "Epoch 6/100, Loss: 0.4465869438080561\n",
      "Epoch 7/100, Loss: 0.38965034892871264\n",
      "Epoch 8/100, Loss: 0.34367997979833964\n",
      "Epoch 9/100, Loss: 0.2988298810308888\n",
      "Epoch 10/100, Loss: 0.25975156477874234\n",
      "Epoch 11/100, Loss: 0.21104901381546542\n",
      "Epoch 12/100, Loss: 0.18513008011948495\n",
      "Epoch 13/100, Loss: 0.15892977817427545\n",
      "Epoch 14/100, Loss: 0.13063457914228951\n",
      "Epoch 15/100, Loss: 0.11774414992846903\n",
      "Epoch 16/100, Loss: 0.10212992045230099\n",
      "Epoch 17/100, Loss: 0.09561726632749751\n",
      "Epoch 18/100, Loss: 0.07826983893201464\n",
      "Epoch 19/100, Loss: 0.0694471117153409\n",
      "Epoch 20/100, Loss: 0.06277466272669179\n",
      "Epoch 21/100, Loss: 0.05312046602678796\n",
      "Epoch 22/100, Loss: 0.04540835405212073\n",
      "Epoch 23/100, Loss: 0.03908882886614828\n",
      "Epoch 24/100, Loss: 0.03972297338657968\n",
      "Epoch 25/100, Loss: 0.03757062776102906\n",
      "Epoch 26/100, Loss: 0.028446307433547362\n",
      "Epoch 27/100, Loss: 0.030029171925326364\n",
      "Epoch 28/100, Loss: 0.026158908897611712\n",
      "Epoch 29/100, Loss: 0.024477988060209014\n",
      "Epoch 30/100, Loss: 0.024888620418052943\n",
      "Epoch 31/100, Loss: 0.02531740511490387\n",
      "Epoch 32/100, Loss: 0.02184488343274487\n",
      "Epoch 33/100, Loss: 0.01950219403564309\n",
      "Epoch 34/100, Loss: 0.017781645095618887\n",
      "Epoch 35/100, Loss: 0.01659346776709537\n",
      "Epoch 36/100, Loss: 0.01382418179751507\n",
      "Epoch 37/100, Loss: 0.01761055368253784\n",
      "Epoch 38/100, Loss: 0.017455317783363473\n",
      "Epoch 39/100, Loss: 0.01221925567439203\n",
      "Epoch 40/100, Loss: 0.012907432318787045\n",
      "Epoch 41/100, Loss: 0.011767194682588092\n",
      "Epoch 42/100, Loss: 0.010295297292205283\n",
      "Epoch 43/100, Loss: 0.011593974608417955\n",
      "Epoch 44/100, Loss: 0.008777764815715187\n",
      "Epoch 45/100, Loss: 0.009253594535125774\n",
      "Epoch 46/100, Loss: 0.008375787156096305\n",
      "Epoch 47/100, Loss: 0.007931343936693988\n",
      "Epoch 48/100, Loss: 0.009886631620160881\n",
      "Epoch 49/100, Loss: 0.007257685605769179\n",
      "Epoch 50/100, Loss: 0.007400979763284947\n",
      "Epoch 51/100, Loss: 0.006857072193053595\n",
      "Epoch 52/100, Loss: 0.006276060576229135\n",
      "Epoch 53/100, Loss: 0.006952671579451167\n",
      "Epoch 54/100, Loss: 0.007710672859545974\n",
      "Epoch 55/100, Loss: 0.008250458075573468\n",
      "Epoch 56/100, Loss: 0.00560466733059868\n",
      "Epoch 57/100, Loss: 0.007268069404138562\n",
      "Epoch 58/100, Loss: 0.007287463100337412\n",
      "Epoch 59/100, Loss: 0.00707257774567032\n",
      "Epoch 60/100, Loss: 0.005709570426136322\n",
      "Epoch 61/100, Loss: 0.004877015380424425\n",
      "Epoch 62/100, Loss: 0.004794645635965502\n",
      "Epoch 63/100, Loss: 0.007228334025144466\n",
      "Epoch 64/100, Loss: 0.0045968202470075565\n",
      "Epoch 65/100, Loss: 0.0050618512600305535\n",
      "Epoch 66/100, Loss: 0.005768172719745919\n",
      "Epoch 67/100, Loss: 0.004360542124881509\n",
      "Epoch 68/100, Loss: 0.004554713493451432\n",
      "Epoch 69/100, Loss: 0.004441420343937352\n",
      "Epoch 70/100, Loss: 0.004166731893617127\n",
      "Epoch 71/100, Loss: 0.0047964810042425855\n",
      "Epoch 72/100, Loss: 0.004525968475458545\n",
      "Epoch 73/100, Loss: 0.004396919952948035\n",
      "Epoch 74/100, Loss: 0.004272840888353087\n",
      "Epoch 75/100, Loss: 0.0042924552035401575\n",
      "Epoch 76/100, Loss: 0.005199571951899478\n",
      "Epoch 77/100, Loss: 0.004002333405618889\n",
      "Epoch 78/100, Loss: 0.003840928751395993\n",
      "Epoch 79/100, Loss: 0.003864394538617316\n",
      "Epoch 80/100, Loss: 0.00488893977793244\n",
      "Epoch 81/100, Loss: 0.004545444310982323\n",
      "Epoch 82/100, Loss: 0.0037585253840439863\n",
      "Epoch 83/100, Loss: 0.004638365389837418\n",
      "Epoch 84/100, Loss: 0.003681561892638759\n",
      "Epoch 85/100, Loss: 0.002839908252229049\n",
      "Epoch 86/100, Loss: 0.004028540959725866\n",
      "Epoch 87/100, Loss: 0.004248137092439546\n",
      "Epoch 88/100, Loss: 0.003405013739774447\n",
      "Epoch 89/100, Loss: 0.0037960688655619464\n",
      "Epoch 90/100, Loss: 0.0033601966747699493\n",
      "Epoch 91/100, Loss: 0.0031061002713007233\n",
      "Epoch 92/100, Loss: 0.003142185081482499\n",
      "Epoch 93/100, Loss: 0.0026527731908052893\n",
      "Epoch 94/100, Loss: 0.003059885794105607\n",
      "Epoch 95/100, Loss: 0.0035288530684954907\n",
      "Epoch 96/100, Loss: 0.003167497019863471\n",
      "Epoch 97/100, Loss: 0.003081416074045202\n",
      "Epoch 98/100, Loss: 0.0024656873766140223\n",
      "Epoch 99/100, Loss: 0.003089020199175658\n",
      "Epoch 100/100, Loss: 0.0034442006610680394\n",
      "Image-Only Model - Precision: 0.1429, Recall: 0.1111, F1-Score: 0.1250\n",
      "Test Accuracy: 33.333333333333336%\n",
      "Precision: 0.14285714285714285\n",
      "Recall: 0.1111111111111111\n",
      "F1-Score: 0.125\n"
     ]
    }
   ],
   "source": [
    "# Experiment: Train Clinical-only and Image-only Models\n",
    "for modality, feature_set in [('Clinical', train_clinical_embeddings), ('Image', train_image_features)]:\n",
    "    print(f\"\\nTraining {modality}-Only Model\")\n",
    "    \n",
    "    train_labels = train_labels.clone().detach().float().view(-1, 1)\n",
    "    test_labels = test_labels.clone().detach().float().view(-1, 1)\n",
    "    train_features = torch.tensor(feature_set.reshape(len(feature_set), -1))\n",
    "    test_features = torch.tensor((test_clinical_embeddings if modality == 'Clinical' else test_image_features).reshape(len(test_labels), -1))\n",
    "\n",
    "    print(\"Train Features: \", train_features.shape)\n",
    "    print(\"Test Features: \", test_features.shape)\n",
    "    print(\"Train Labels: \", train_labels.shape)\n",
    "    print(\"Test Labels: \", test_labels.shape)\n",
    "    \n",
    "    train_dataset = TensorDataset(train_features, train_labels)\n",
    "    test_dataset = TensorDataset(test_features, test_labels)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=1, shuffle=False)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "    \n",
    "    model = MLP(input_dim=train_features.shape[1])\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=w_decay)\n",
    "    \n",
    "    epochs = 100\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for features, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(features.float())\n",
    "\n",
    "            loss = torch.nn.BCEWithLogitsLoss()(output.view(-1), labels.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss/len(train_loader)}\")\n",
    "    \n",
    "    model.eval()\n",
    "    all_labels, all_predictions = [], []\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for features, labels in test_loader:\n",
    "            output = model(features.float())\n",
    "\n",
    "            pred = torch.sigmoid(output.squeeze()) >= 0.5\n",
    "            \n",
    "            all_labels.append(labels.cpu().numpy().flatten())\n",
    "            all_predictions.append(pred.cpu().numpy().flatten())\n",
    "\n",
    "            # Count correct predictions\n",
    "            correct += (pred == labels).sum().item()\n",
    "            total += labels.size(0)  # Increment by the number of samples in this batch\n",
    "    \n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    print(f\"{modality}-Only Model - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Accuracy\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy}%\")\n",
    "\n",
    "    # Calculate Metrics\n",
    "    precision = precision_score(all_labels, all_predictions)\n",
    "    recall = recall_score(all_labels, all_predictions)\n",
    "    f1 = f1_score(all_labels, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_labels, all_predictions)\n",
    "\n",
    "    print(f\"Precision: {precision}\")\n",
    "    print(f\"Recall: {recall}\")\n",
    "    print(f\"F1-Score: {f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hackathon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
